; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -S -passes=vector-combine -mtriple=aarch64 < %s | FileCheck %s

define i1 @or_eq_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @or_eq_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @or_ne_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @or_ne_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 0
  ret i1 %cmp
}

define i1 @or_eq_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @or_eq_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 1
  ret i1 %cmp
}

define i1 @or_ne_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @or_ne_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 1
  ret i1 %cmp
}

define i1 @umax_eq_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @umax_eq_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @umax_ne_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @umax_ne_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 0
  ret i1 %cmp
}

define i1 @umax_eq_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @umax_eq_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 1
  ret i1 %cmp
}

define i1 @umax_ne_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @umax_ne_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 1
  ret i1 %cmp
}

define i1 @and_eq_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @and_eq_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @and_ne_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @and_ne_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 0
  ret i1 %cmp
}

define i1 @and_eq_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @and_eq_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 1
  ret i1 %cmp
}

define i1 @and_ne_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @and_ne_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 1
  ret i1 %cmp
}

define i1 @umin_eq_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @umin_eq_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @umin_ne_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @umin_ne_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 0
  ret i1 %cmp
}

define i1 @umin_eq_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @umin_eq_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 1
  ret i1 %cmp
}

define i1 @umin_ne_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @umin_ne_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 1
  ret i1 %cmp
}

define i1 @add_eq_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @add_eq_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @add_ne_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @add_ne_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 0
  ret i1 %cmp
}

define i1 @add_eq_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @add_eq_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 4
  ret i1 %cmp
}

define i1 @add_ne_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @add_ne_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, 4
  ret i1 %cmp
}

define i1 @add_ult_max(<4 x i32> %x) {
; CHECK-LABEL: define i1 @add_ult_max(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp ult i32 %red, 4
  ret i1 %cmp
}

define i1 @add_ugt_max_minus_1(<4 x i32> %x) {
; CHECK-LABEL: define i1 @add_ugt_max_minus_1(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp ugt i32 %red, 3
  ret i1 %cmp
}

define i1 @ashr_add_eq_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @ashr_add_eq_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @or_eq_0_v8i16(<8 x i16> %x) {
; CHECK-LABEL: define i1 @or_eq_0_v8i16(
; CHECK-SAME: <8 x i16> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i16 @llvm.vector.reduce.umax.v8i16(<8 x i16> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i16 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <8 x i16> %x, splat (i16 15)
  %red = call i16 @llvm.vector.reduce.or.v8i16(<8 x i16> %shr)
  %cmp = icmp eq i16 %red, 0
  ret i1 %cmp
}

define i1 @and_eq_max_v2i64(<2 x i64> %x) {
; CHECK-LABEL: define i1 @and_eq_max_v2i64(
; CHECK-SAME: <2 x i64> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i64 @llvm.vector.reduce.umin.v2i64(<2 x i64> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i64 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <2 x i64> %x, splat (i64 63)
  %red = call i64 @llvm.vector.reduce.and.v2i64(<2 x i64> %shr)
  %cmp = icmp eq i64 %red, 1
  ret i1 %cmp
}

; negative: shift amount is not bitwidth-1
define i1 @negative_wrong_shift(<4 x i32> %x) {
; CHECK-LABEL: define i1 @negative_wrong_shift(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i32> [[X]], splat (i32 30)
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 30)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

; negative: comparison constant is neither 0 nor max
define i1 @negative_wrong_cmp_const(<4 x i32> %x) {
; CHECK-LABEL: define i1 @negative_wrong_cmp_const(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i32> [[X]], splat (i32 31)
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i32 [[RED]], 2
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 2
  ret i1 %cmp
}

; negative: shift has multiple uses
define i1 @negative_multi_use_shift(<4 x i32> %x, ptr %p) {
; CHECK-LABEL: define i1 @negative_multi_use_shift(
; CHECK-SAME: <4 x i32> [[X:%.*]], ptr [[P:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i32> [[X]], splat (i32 31)
; CHECK-NEXT:    store <4 x i32> [[SHR]], ptr [[P]], align 16
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  store <4 x i32> %shr, ptr %p
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

; negative: sgt with wrong constant (not 0 or max-1)
define i1 @negative_sgt_wrong_const(<4 x i32> %x) {
; CHECK-LABEL: define i1 @negative_sgt_wrong_const(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i32> [[X]], splat (i32 31)
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[RED]], 1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp sgt i32 %red, 1
  ret i1 %cmp
}

; negative: slt with wrong constant (not 1 or max)
define i1 @negative_slt_wrong_const(<4 x i32> %x) {
; CHECK-LABEL: define i1 @negative_slt_wrong_const(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i32> [[X]], splat (i32 31)
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[RED]], 2
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp slt i32 %red, 2
  ret i1 %cmp
}

; negative: if reduce.add can wrap, transformation is incorrect
define i1 @negative_add_numelts_overflow(<8 x i2> %x) {
; CHECK-LABEL: define i1 @negative_add_numelts_overflow(
; CHECK-SAME: <8 x i2> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <8 x i2> [[X]], splat (i2 1)
; CHECK-NEXT:    [[RED:%.*]] = call i2 @llvm.vector.reduce.add.v8i2(<8 x i2> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i2 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <8 x i2> %x, splat (i2 1)
  %red = call i2 @llvm.vector.reduce.add.v8i2(<8 x i2> %shr)
  %cmp = icmp eq i2 %red, 0
  ret i1 %cmp
}

define i1 @ashr_add_eq_allneg(<4 x i32> %x) {
; CHECK-LABEL: define i1 @ashr_add_eq_allneg(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp eq i32 %red, -4
  ret i1 %cmp
}

define i1 @ashr_add_ne_allneg(<4 x i32> %x) {
; CHECK-LABEL: define i1 @ashr_add_ne_allneg(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp ne i32 %red, -4
  ret i1 %cmp
}

define i1 @ashr_add_sgt_minus1(<4 x i32> %x) {
; CHECK-LABEL: define i1 @ashr_add_sgt_minus1(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp sgt i32 %red, -1
  ret i1 %cmp
}

define i1 @ashr_add_slt_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @ashr_add_slt_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp slt i32 %red, 0
  ret i1 %cmp
}

define i1 @ashr_add_slt_minus3(<4 x i32> %x) {
; CHECK-LABEL: define i1 @ashr_add_slt_minus3(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp slt i32 %red, -3
  ret i1 %cmp
}

define i1 @ashr_add_sgt_minus4(<4 x i32> %x) {
; CHECK-LABEL: define i1 @ashr_add_sgt_minus4(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP1]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp sgt i32 %red, -4
  ret i1 %cmp
}

define i1 @ashr_add_eq_allneg_v8i16(<8 x i16> %x) {
; CHECK-LABEL: define i1 @ashr_add_eq_allneg_v8i16(
; CHECK-SAME: <8 x i16> [[X:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = call i16 @llvm.vector.reduce.umin.v8i16(<8 x i16> [[X]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i16 [[TMP1]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <8 x i16> %x, splat (i16 15)
  %red = call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %shr)
  %cmp = icmp eq i16 %red, -8
  ret i1 %cmp
}

; negative: NumElts=2 doesn't fit as signed in i2
define i1 @add_eq_0_v2i2(<2 x i2> %x) {
; CHECK-LABEL: define i1 @add_eq_0_v2i2(
; CHECK-SAME: <2 x i2> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <2 x i2> [[X]], splat (i2 1)
; CHECK-NEXT:    [[RED:%.*]] = call i2 @llvm.vector.reduce.add.v2i2(<2 x i2> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i2 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <2 x i2> %x, splat (i2 1)
  %red = call i2 @llvm.vector.reduce.add.v2i2(<2 x i2> %shr)
  %cmp = icmp eq i2 %red, 0
  ret i1 %cmp
}

; negative: NumElts=3 doesn't fit as signed in i2
define i1 @add_eq_max_v3i2(<3 x i2> %x) {
; CHECK-LABEL: define i1 @add_eq_max_v3i2(
; CHECK-SAME: <3 x i2> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <3 x i2> [[X]], splat (i2 1)
; CHECK-NEXT:    [[RED:%.*]] = call i2 @llvm.vector.reduce.add.v3i2(<3 x i2> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i2 [[RED]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <3 x i2> %x, splat (i2 1)
  %red = call i2 @llvm.vector.reduce.add.v3i2(<3 x i2> %shr)
  %cmp = icmp eq i2 %red, 3
  ret i1 %cmp
}

; negative: <4 x i2> reduce.add overflows
define i1 @negative_add_v4i2(<4 x i2> %x) {
; CHECK-LABEL: define i1 @negative_add_v4i2(
; CHECK-SAME: <4 x i2> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i2> [[X]], splat (i2 1)
; CHECK-NEXT:    [[RED:%.*]] = call i2 @llvm.vector.reduce.add.v4i2(<4 x i2> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i2 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i2> %x, splat (i2 1)
  %red = call i2 @llvm.vector.reduce.add.v4i2(<4 x i2> %shr)
  %cmp = icmp eq i2 %red, 0
  ret i1 %cmp
}

; negative: ashr with NumElts=5 causes -5 to wrap to 3 (positive),
define i1 @negative_ashr_add_sgt_0(<5 x i3> %x) {
; CHECK-LABEL: define i1 @negative_ashr_add_sgt_0(
; CHECK-SAME: <5 x i3> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = ashr <5 x i3> [[X]], splat (i3 2)
; CHECK-NEXT:    [[RED:%.*]] = call i3 @llvm.vector.reduce.add.v5i3(<5 x i3> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i3 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <5 x i3> %x, splat (i3 2)
  %red = call i3 @llvm.vector.reduce.add.v5i3(<5 x i3> %shr)
  %cmp = icmp sgt i3 %red, 0
  ret i1 %cmp
}

define i1 @i1_or_eq_0(<4 x i1> %x) {
; CHECK-LABEL: define i1 @i1_or_eq_0(
; CHECK-SAME: <4 x i1> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i1> [[X]], zeroinitializer
; CHECK-NEXT:    [[RED:%.*]] = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i1 [[RED]], false
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i1> %x, splat (i1 0)
  %red = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> %shr)
  %cmp = icmp eq i1 %red, 0
  ret i1 %cmp
}

define i1 @i1_or_ne_0(<4 x i1> %x) {
; CHECK-LABEL: define i1 @i1_or_ne_0(
; CHECK-SAME: <4 x i1> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i1> [[X]], zeroinitializer
; CHECK-NEXT:    [[RED:%.*]] = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp ne i1 [[RED]], false
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i1> %x, splat (i1 0)
  %red = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> %shr)
  %cmp = icmp ne i1 %red, 0
  ret i1 %cmp
}

define i1 @i1_and_eq_0(<4 x i1> %x) {
; CHECK-LABEL: define i1 @i1_and_eq_0(
; CHECK-SAME: <4 x i1> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i1> [[X]], zeroinitializer
; CHECK-NEXT:    [[RED:%.*]] = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i1 [[RED]], false
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i1> %x, splat (i1 0)
  %red = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %shr)
  %cmp = icmp eq i1 %red, 0
  ret i1 %cmp
}

define i1 @i1_and_ne_0(<4 x i1> %x) {
; CHECK-LABEL: define i1 @i1_and_ne_0(
; CHECK-SAME: <4 x i1> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i1> [[X]], zeroinitializer
; CHECK-NEXT:    [[RED:%.*]] = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp ne i1 [[RED]], false
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i1> %x, splat (i1 0)
  %red = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %shr)
  %cmp = icmp ne i1 %red, 0
  ret i1 %cmp
}

define i1 @i1_umax_eq_0(<4 x i1> %x) {
; CHECK-LABEL: define i1 @i1_umax_eq_0(
; CHECK-SAME: <4 x i1> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i1> [[X]], zeroinitializer
; CHECK-NEXT:    [[RED:%.*]] = call i1 @llvm.vector.reduce.umax.v4i1(<4 x i1> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i1 [[RED]], false
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i1> %x, splat (i1 0)
  %red = call i1 @llvm.vector.reduce.umax.v4i1(<4 x i1> %shr)
  %cmp = icmp eq i1 %red, 0
  ret i1 %cmp
}

define i1 @i1_umin_ne_0(<4 x i1> %x) {
; CHECK-LABEL: define i1 @i1_umin_ne_0(
; CHECK-SAME: <4 x i1> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = lshr <4 x i1> [[X]], zeroinitializer
; CHECK-NEXT:    [[RED:%.*]] = call i1 @llvm.vector.reduce.umin.v4i1(<4 x i1> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp ne i1 [[RED]], false
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = lshr <4 x i1> %x, splat (i1 0)
  %red = call i1 @llvm.vector.reduce.umin.v4i1(<4 x i1> %shr)
  %cmp = icmp ne i1 %red, 0
  ret i1 %cmp
}

define i1 @i1_ashr_or_eq_0(<4 x i1> %x) {
; CHECK-LABEL: define i1 @i1_ashr_or_eq_0(
; CHECK-SAME: <4 x i1> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = ashr <4 x i1> [[X]], zeroinitializer
; CHECK-NEXT:    [[RED:%.*]] = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i1 [[RED]], false
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i1> %x, splat (i1 0)
  %red = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> %shr)
  %cmp = icmp eq i1 %red, 0
  ret i1 %cmp
}

define i1 @i1_ashr_and_ne_0(<4 x i1> %x) {
; CHECK-LABEL: define i1 @i1_ashr_and_ne_0(
; CHECK-SAME: <4 x i1> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = ashr <4 x i1> [[X]], zeroinitializer
; CHECK-NEXT:    [[RED:%.*]] = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp ne i1 [[RED]], false
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i1> %x, splat (i1 0)
  %red = call i1 @llvm.vector.reduce.and.v4i1(<4 x i1> %shr)
  %cmp = icmp ne i1 %red, 0
  ret i1 %cmp
}

define i1 @negative_ashr_add_ult_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @negative_ashr_add_ult_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = ashr <4 x i32> [[X]], splat (i32 31)
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp ult i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp ult i32 %red, 0
  ret i1 %cmp
}

define i1 @negative_ashr_add_ugt_minus1(<4 x i32> %x) {
; CHECK-LABEL: define i1 @negative_ashr_add_ugt_minus1(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = ashr <4 x i32> [[X]], splat (i32 31)
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp ugt i32 [[RED]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp ugt i32 %red, -1
  ret i1 %cmp
}

define i1 @negative_ashr_add_ult_minus3(<4 x i32> %x) {
; CHECK-LABEL: define i1 @negative_ashr_add_ult_minus3(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = ashr <4 x i32> [[X]], splat (i32 31)
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp ult i32 [[RED]], -3
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %shr)
  %cmp = icmp ult i32 %red, -3
  ret i1 %cmp
}

define i1 @negative_ashr_or_ult_0(<4 x i32> %x) {
; CHECK-LABEL: define i1 @negative_ashr_or_ult_0(
; CHECK-SAME: <4 x i32> [[X:%.*]]) {
; CHECK-NEXT:    [[SHR:%.*]] = ashr <4 x i32> [[X]], splat (i32 31)
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> [[SHR]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp ult i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %shr = ashr <4 x i32> %x, splat (i32 31)
  %red = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %shr)
  %cmp = icmp ult i32 %red, 0
  ret i1 %cmp
}

define i1 @multi_add_lshr_eq_0(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_add_lshr_eq_0(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = or <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP2]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @multi_add_lshr_ne_0(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_add_lshr_ne_0(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = or <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp ne i32 %red, 0
  ret i1 %cmp
}

define i1 @multi_add_lshr_eq_8(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_add_lshr_eq_8(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = and <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp eq i32 %red, 8
  ret i1 %cmp
}

define i1 @multi_add_lshr_ne_8(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_add_lshr_ne_8(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = and <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP2]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp ne i32 %red, 8
  ret i1 %cmp
}

define i1 @multi_or_lshr_eq_0(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_or_lshr_eq_0(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = or <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP2]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %combined = or <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %combined)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @multi_and_lshr_eq_1(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_and_lshr_eq_1(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = and <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %combined = and <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.and.v4i32(<4 x i32> %combined)
  %cmp = icmp eq i32 %red, 1
  ret i1 %cmp
}

define i1 @multi_triple_add_lshr_eq_0(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c) {
; CHECK-LABEL: define i1 @multi_triple_add_lshr_eq_0(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]], <4 x i32> [[C:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = or <4 x i32> [[C]], [[B]]
; CHECK-NEXT:    [[TMP2:%.*]] = or <4 x i32> [[TMP1]], [[A]]
; CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[TMP2]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP3]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %sc = lshr <4 x i32> %c, splat (i32 31)
  %ab = add <4 x i32> %sa, %sb
  %abc = add <4 x i32> %ab, %sc
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %abc)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @multi_triple_add_lshr_eq_12(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c) {
; CHECK-LABEL: define i1 @multi_triple_add_lshr_eq_12(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]], <4 x i32> [[C:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = and <4 x i32> [[C]], [[B]]
; CHECK-NEXT:    [[TMP2:%.*]] = and <4 x i32> [[TMP1]], [[A]]
; CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[TMP2]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP3]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %sc = lshr <4 x i32> %c, splat (i32 31)
  %ab = add <4 x i32> %sa, %sb
  %abc = add <4 x i32> %ab, %sc
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %abc)
  %cmp = icmp eq i32 %red, 12
  ret i1 %cmp
}

define i1 @multi_add_ashr_eq_0(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_add_ashr_eq_0(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = or <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP2]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = ashr <4 x i32> %a, splat (i32 31)
  %sb = ashr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @multi_add_ashr_eq_minus8(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_add_ashr_eq_minus8(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = and <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = ashr <4 x i32> %a, splat (i32 31)
  %sb = ashr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp eq i32 %red, -8
  ret i1 %cmp
}

define i1 @multi_add_ashr_sgt_minus1(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_add_ashr_sgt_minus1(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = or <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP2]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = ashr <4 x i32> %a, splat (i32 31)
  %sb = ashr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp sgt i32 %red, -1
  ret i1 %cmp
}

define i1 @multi_add_ashr_slt_minus7(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_add_ashr_slt_minus7(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = and <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = ashr <4 x i32> %a, splat (i32 31)
  %sb = ashr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp slt i32 %red, -7
  ret i1 %cmp
}

define i1 @multi_umax_or_tree_eq_0(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_umax_or_tree_eq_0(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = or <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp sgt i32 [[TMP2]], -1
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %combined = or <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> %combined)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

define i1 @multi_umin_and_tree_eq_1(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @multi_umin_and_tree_eq_1(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = and <4 x i32> [[B]], [[A]]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> [[TMP1]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt i32 [[TMP2]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %combined = and <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.umin.v4i32(<4 x i32> %combined)
  %cmp = icmp eq i32 %red, 1
  ret i1 %cmp
}

; negative: mixed lshr/ashr shifts
define i1 @negative_multi_mixed_shifts(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @negative_multi_mixed_shifts(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[SA:%.*]] = lshr <4 x i32> [[A]], splat (i32 31)
; CHECK-NEXT:    [[SB:%.*]] = ashr <4 x i32> [[B]], splat (i32 31)
; CHECK-NEXT:    [[SUM:%.*]] = add <4 x i32> [[SA]], [[SB]]
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SUM]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = ashr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

; negative: overflow with two vectors (2*8 = 16 > max for i2)
define i1 @negative_multi_overflow(<8 x i2> %a, <8 x i2> %b) {
; CHECK-LABEL: define i1 @negative_multi_overflow(
; CHECK-SAME: <8 x i2> [[A:%.*]], <8 x i2> [[B:%.*]]) {
; CHECK-NEXT:    [[SA:%.*]] = lshr <8 x i2> [[A]], splat (i2 1)
; CHECK-NEXT:    [[SB:%.*]] = lshr <8 x i2> [[B]], splat (i2 1)
; CHECK-NEXT:    [[SUM:%.*]] = add <8 x i2> [[SA]], [[SB]]
; CHECK-NEXT:    [[RED:%.*]] = call i2 @llvm.vector.reduce.add.v8i2(<8 x i2> [[SUM]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i2 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <8 x i2> %a, splat (i2 1)
  %sb = lshr <8 x i2> %b, splat (i2 1)
  %sum = add <8 x i2> %sa, %sb
  %red = call i2 @llvm.vector.reduce.add.v8i2(<8 x i2> %sum)
  %cmp = icmp eq i2 %red, 0
  ret i1 %cmp
}

; negative: shift has multiple uses
define i1 @negative_multi_shift_multiuse(<4 x i32> %a, <4 x i32> %b, ptr %p) {
; CHECK-LABEL: define i1 @negative_multi_shift_multiuse(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]], ptr [[P:%.*]]) {
; CHECK-NEXT:    [[SA:%.*]] = lshr <4 x i32> [[A]], splat (i32 31)
; CHECK-NEXT:    store <4 x i32> [[SA]], ptr [[P]], align 16
; CHECK-NEXT:    [[SB:%.*]] = lshr <4 x i32> [[B]], splat (i32 31)
; CHECK-NEXT:    [[SUM:%.*]] = add <4 x i32> [[SA]], [[SB]]
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SUM]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  store <4 x i32> %sa, ptr %p
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sum)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

; negative: internal tree node has multiple uses
define i1 @negative_multi_tree_node_multiuse(<4 x i32> %a, <4 x i32> %b, <4 x i32> %c, ptr %p) {
; CHECK-LABEL: define i1 @negative_multi_tree_node_multiuse(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]], <4 x i32> [[C:%.*]], ptr [[P:%.*]]) {
; CHECK-NEXT:    [[SA:%.*]] = lshr <4 x i32> [[A]], splat (i32 31)
; CHECK-NEXT:    [[SB:%.*]] = lshr <4 x i32> [[B]], splat (i32 31)
; CHECK-NEXT:    [[SC:%.*]] = lshr <4 x i32> [[C]], splat (i32 31)
; CHECK-NEXT:    [[AB:%.*]] = add <4 x i32> [[SA]], [[SB]]
; CHECK-NEXT:    store <4 x i32> [[AB]], ptr [[P]], align 16
; CHECK-NEXT:    [[ABC:%.*]] = add <4 x i32> [[AB]], [[SC]]
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[ABC]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %sc = lshr <4 x i32> %c, splat (i32 31)
  %ab = add <4 x i32> %sa, %sb
  store <4 x i32> %ab, ptr %p
  %abc = add <4 x i32> %ab, %sc
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %abc)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

; negative: tree op (add) doesn't match reduction (or/umax expects or tree)
define i1 @negative_multi_op_mismatch(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @negative_multi_op_mismatch(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) {
; CHECK-NEXT:    [[SA:%.*]] = lshr <4 x i32> [[A]], splat (i32 31)
; CHECK-NEXT:    [[SB:%.*]] = lshr <4 x i32> [[B]], splat (i32 31)
; CHECK-NEXT:    [[SUM:%.*]] = add <4 x i32> [[SA]], [[SB]]
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.umax.v4i32(<4 x i32> [[SUM]])
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[CMP]]
;
  %sa = lshr <4 x i32> %a, splat (i32 31)
  %sb = lshr <4 x i32> %b, splat (i32 31)
  %sum = add <4 x i32> %sa, %sb
  %red = call i32 @llvm.vector.reduce.or.v4i32(<4 x i32> %sum)
  %cmp = icmp eq i32 %red, 0
  ret i1 %cmp
}

